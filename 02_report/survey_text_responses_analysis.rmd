---
title: "Survey Text Analysis"
author: "Chris Selig"
date: "`r Sys.Date()`"
output: 
    tufte::tufte_html: default

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = F,
    warning = F,
    message = F,
    fig.align = 'center'
    )
```

```{r load libraries}
# rmd theme
library(tufte)

# Workhorse: data manipulation, plotting
library(tidyverse)
library(janitor)
library(reshape2)

# Read in data
library(readxl)

# Text specific packages
library(tidytext)
library(textdata)
library(stopwords)
library(sentimentr)

# Visualization packages
library(ggthemes)
library(wordcloud)
library(DT)
```


```{r load data}
raw_data <- read_excel("../00_data/survey_text_responses.xlsx",sheet = "Survey_responses")
keyword <- read_excel("../00_data/keywords.xlsx")
```

```{r source scripts}
source("../01_scripts/01_data_processing.R")
source("../01_scripts/02_visualizations.R")
```

```{r set theme for plots}
# Set theme
theme_set(theme_tufte())
```


```{r loadFonts}
# Load custom fonts
windowsFonts(memphis=windowsFont("memphis"))
windowsFonts(ArnoProLightDisplay = windowsFont("Arno Pro Light Display"))
```


```{r unigrams data processing}
unigram_tidy_data <- raw_data %>% 
    clean_column_names_function() %>% 
    tokenize_data_function() %>% 
    remove_stop_words_function()
```

## Purpose

The purpose of this analysis is to analyze the four open ended questions in a survey for customer satisfaction for VHT's products.

The four questions are:

1. If you could change anything about the services VHT Studios offers, what would it be? Tagged as "Change 1 thing" in dataset.

2. What can we do better? Tagged as "do better" in dataset.

3. What other capabilities would you be interested in VHT offering? Tagged as "Other Services" in dataset.

4. Additional Comments and/or Suggestions for improvements or changes. Tagged as "Other Comments" in dataset.


## Most Common Words Overall

First, I look at the most common words overall. Note, stop words are removed. Stop words are extremely common words like "and", "a", and "the" and offer no meaning for analytical purposes.

```{r}
set.seed(100)

unigram_tidy_data %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words = 100))
```


The wordcloud above shows the top 100 words used by the respondents. Photographer is the most frequent word, which really isn't a big surprise, but some other interesting ones are 'service', 'time','pricing' and 'quicker'.


## Single Word Sentiment Wordcloud

Below, the top 150 words are visualized based on sentiment.


```{r}
unigram_tidy_data %>% 
    inner_join(get_sentiments("bing")) %>%
    count(word, sentiment, sort = TRUE) %>%
    acast(word ~ sentiment, value.var = "n", fill = 0) %>%
    comparison.cloud(
    colors = c("#CD5C5C", "darkgreen"),
    title.bg.colors = NA,
    max.words = 150
  )
```


## Visualize Most Common Single Words by Question


The below image shows the single most frequent words that appear on each of the four open ended questions.


Some observations:

1. Pricing seems to be pretty popular in the "change 1 thing" question as it appears twice, as price and pricing.

2. Photos appears the most times in both the "change 1 thing" and "do better" questions. That would be something else to explore. 

3. The "other services" questions has some interesting words, like video and drone. Both are services offered so it would be interesting to see the context.


```{r unigram visual}
barchart_function(
    data = unigram_tidy_data, 
    title = "Top 10 Single Words for Each Open Question",
    subtitle = "More than 10 items appear due to ties"
    )
```


## Common Bigrams - Two words that appear side by side

The next section will show two words that often appear together. Note, bigrams appear less frequently than single words so counts will be lower.


```{r bigrams data processing}
bigram_tidy_data <- raw_data %>% 
    clean_column_names_function() %>% 
    bigram_data_function()
```

```{r bigram visual}
bigram_tidy_data %>% 
    barchart_function(
        title = "Top 10 Bigrams by Question",
        subtitle = "More than 10 bigrams appear due to ties"
    )
```

Bigrams are a little more interesting than unigrams because you can start to see some context around the verbage in the responses.

1. Virtual staging appers most frequently in the "change 1 thing" and other services questions, while it is second place in the "do better" and "other comments" questions.

2. "Gold package" makes the top 10 in the "do better" question.

3. "Customer service" is number 1 in "do better"

4. "Floor plan(s)" appears twice in the "change 1 thing" question


## Sentence Sentiment

Lookng at the sentiment of individual words is a bit too primitive and the English language is syntactically complex and lexically rich so now I will take a look at the sentiment of full responses. 

The algorithm I will use is a bit better than sentiment by word. It uses "valence shifters" that help adjust the sentiment score. For example, if you do sentiment analysis on the single word "happy", the score is positive. Obviously though, if the phrase is "not happy" it is no longer positive but a single word sentiment analysis would not pick that up. The valence shifters will help adjust "not happy" to negative or at least less positive. Also, the sentiment score that will be returned will be an aggregate of each sentence for each question and response. That way we only look at a single score per question and response. A score greater than 0 implies an overall positive sentiment.

A note on sentiment, it is not a perfect process. Words are normally crowdsourced to see if they are positive or negative, so the context a word is used may not be included. A good example is the word "plot". A movie plot is a positive word, but a plot to do a bad deed is negative.

A more specific example for this dataset is this response:

"$150 for 20 photos is a lot of money when we can get cheaper pricing in our area."

This is scored as 0.4 which is a positive score but it isn't a positive response by the recipient.

In the table below you can search both by score and whether the sentiment is positive or negative.

```{r calcSentiment}
sentence_sentiment <- raw_data %>% 
    clean_column_names_function() %>% 
    select(starts_with("open")) %>% 
        pivot_longer(
            cols = c(1:4),
            names_to = "question",
            values_to = "text"
        ) %>% 
    filter(!is.na(text)) %>% 
    mutate(
        question = str_replace(question, "open_",""),
        question = str_replace_all(question, "_"," "),
        question = str_to_title(question)
    ) %>% 
    group_by(question) %>%
    unnest_tokens(sentence, text, token = 'sentences') %>%
    ungroup() %>%
    get_sentences() %>%
    sentiment_by(by = c('question', 'sentence')) %>% 
    filter(ave_sentiment != 0.0) %>% 
    select(-c(sd,word_count)) %>% 
    rename(`Avg Sentiment` = ave_sentiment) %>% 
    mutate(`Avg Sentiment` = round(`Avg Sentiment`,2)) %>% 
    mutate(Sentiment = ifelse(`Avg Sentiment` > 0, 'positive','negative'))

```

```{r visualizeSentenceSentiment}
sentence_sentiment %>% 
    datatable(options = list(pageLength = 10))
```


## Keyword Search

In the table below, you can search for a keyword and find the responses that include that word and read the full responses.

```{r}
raw_data %>% 
    clean_column_names_function() %>% 
    select(respondent_id,starts_with("open")) %>% 
    pivot_longer(
        cols = c(2:5),
        names_to = "question",
        values_to = "text"
    ) %>% 
    filter(!is.na(text)) %>% 
    clean_question_column_function() %>% 
    select(-respondent_id) %>% 
    datatable(options = list(pageLength = 10))
    
```


## Word Correlations

## Topic Modeling